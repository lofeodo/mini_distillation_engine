{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e9fff13c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Environment Sanity Check ===\n",
      "Python version: 3.10.19 | packaged by Anaconda, Inc. | (main, Oct 21 2025, 16:41:31) [MSC v.1929 64 bit (AMD64)]\n",
      "Platform: Windows-10-10.0.26200-SP0\n",
      "PyTorch version: 2.5.1\n",
      "Transformers version: 5.1.0\n",
      "Pydantic version: 2.12.5\n",
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "# Environment sanity check\n",
    "import sys\n",
    "import torch\n",
    "import transformers\n",
    "import pydantic\n",
    "import json\n",
    "import platform\n",
    "\n",
    "print(\"=== Environment Sanity Check ===\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Platform: {platform.platform()}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Transformers version: {transformers.__version__}\")\n",
    "print(f\"Pydantic version: {pydantic.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fce4ebb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading tokenizer: microsoft/Phi-3-mini-4k-instruct\n",
      "Loading model: microsoft/Phi-3-mini-4k-instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 195/195 [00:16<00:00, 11.90it/s, Materializing param=model.norm.weight]                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and tokenizer loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "MODEL_NAME = \"microsoft/Phi-3-mini-4k-instruct\"  # or your chosen local model\n",
    "\n",
    "print(f\"Loading tokenizer: {MODEL_NAME}\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "\n",
    "print(f\"Loading model: {MODEL_NAME}\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    torch_dtype=torch.float32,\n",
    "    device_map=\"cpu\",\n",
    ")\n",
    "\n",
    "print(\"Model and tokenizer loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9809e444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: Test input: patient has systolic BP > 180 mmHg.\n",
      "Decoded : Test input: patient has systolic BP > 180 mmHg.\n"
     ]
    }
   ],
   "source": [
    "text = \"Test input: patient has systolic BP > 180 mmHg.\"\n",
    "tokens = tokenizer(text, return_tensors=\"pt\")\n",
    "decoded = tokenizer.decode(tokens[\"input_ids\"][0])\n",
    "\n",
    "print(\"Original:\", text)\n",
    "print(\"Decoded :\", decoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0dfd9a8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pydantic validation: PASSED\n",
      "{\n",
      "  \"fact_type\": \"threshold\",\n",
      "  \"text\": \"Systolic blood pressure \\u2265 180 mmHg requires urgent evaluation.\",\n",
      "  \"citations\": [\n",
      "    {\n",
      "      \"chunk_id\": \"guideline_01\",\n",
      "      \"line_start\": 12,\n",
      "      \"line_end\": 14\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field, ValidationError\n",
    "from typing import List\n",
    "\n",
    "class Citation(BaseModel):\n",
    "    chunk_id: str\n",
    "    line_start: int\n",
    "    line_end: int\n",
    "\n",
    "class ExtractedFact(BaseModel):\n",
    "    fact_type: str\n",
    "    text: str\n",
    "    citations: List[Citation]\n",
    "\n",
    "sample = {\n",
    "    \"fact_type\": \"threshold\",\n",
    "    \"text\": \"Systolic blood pressure ≥ 180 mmHg requires urgent evaluation.\",\n",
    "    \"citations\": [\n",
    "        {\"chunk_id\": \"guideline_01\", \"line_start\": 12, \"line_end\": 14}\n",
    "    ],\n",
    "}\n",
    "\n",
    "try:\n",
    "    fact = ExtractedFact(**sample)\n",
    "    print(\"Pydantic validation: PASSED\")\n",
    "    print(json.dumps(fact.model_dump(), indent=2))\n",
    "except ValidationError as e:\n",
    "    print(\"Pydantic validation: FAILED\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6f284692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extract explicitly stated clinical thresholds only.\n",
      "\n",
      "Document:\n",
      "\n",
      "Clinical Trial: Efficacy of Drug X in Treating Chronic Pain\n",
      "\n",
      "Background:\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Extract explicitly stated clinical thresholds only.\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=30,\n",
    "        do_sample=False,\n",
    "    )\n",
    "\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
